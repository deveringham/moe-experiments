{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c9b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35349f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94909636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c91d67f06a74c15813a203ca118c6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B-Chat\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118693f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence that is designed to understand and generate human-like language. These models are typically very large in terms of their size, often consisting of billions of parameters, and are trained on vast amounts of text data in order to learn the patterns and structures of language.\n",
      "\n",
      "Large language models are used in a variety of applications, including natural language processing (NLP) tasks such as language translation, sentiment analysis, and chatbot development. They can also be used for text generation, such as generating creative writing or summarizing long documents.\n",
      "\n",
      "One of the key advantages of large language models is their ability to handle complex language tasks, as they have been trained on a wide range of language data and have learned to understand the nuances of language. However, this also means that they can sometimes produce unexpected or nonsensical output, which requires careful monitoring and tuning when they are used in real-world applications.\n",
      "\n",
      "Overall, large language models are an important tool in the field of AI, enabling advances in areas such as NLP and machine translation, and opening up new possibilities for natural language-based interactions with computers.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c152d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "100%|██████████████████████████████████████| 1000/1000 [00:02<00:00, 420.53it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1336.37it/s]\n",
      "Running loglikelihood requests: 100%|███████| 7997/7997 [22:27<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arc_easy: {'alias': 'arc_easy', 'acc,none': 0.703, 'acc_stderr,none': 0.014456832294801103, 'acc_norm,none': 0.679, 'acc_norm_stderr,none': 0.014770821817934638}\n",
      "hellaswag: {'alias': 'hellaswag', 'acc,none': 0.515, 'acc_stderr,none': 0.015812179641814895, 'acc_norm,none': 0.669, 'acc_norm_stderr,none': 0.014888272588203941}\n"
     ]
    }
   ],
   "source": [
    "eval_model = HFLM(\n",
    "    pretrained=model, \n",
    "    device=device,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Choose tasks (Hellaswag for commonsense, ARC-Easy for reasoning)\n",
    "tasks = [\"hellaswag\", \"arc_easy\"]\n",
    "\n",
    "# Run evaluation\n",
    "results = lm_eval.simple_evaluate(\n",
    "    model=eval_model,\n",
    "    tasks=tasks,\n",
    "    num_fewshot=0,  # Zero-shot evaluation\n",
    "    limit=1000      # Limit samples for quick testing\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for task, metrics in results[\"results\"].items():\n",
    "    print(f\"{task}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f92fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
