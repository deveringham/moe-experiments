{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e24817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ed4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print(\"func:%r args:[%r, %r] took: %2.4f sec\" % (f.__name__, args, kw, te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd1f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoE monitoring hook. Attaches a callback to routing modules which computes routing metrics\n",
    "\n",
    "class MoEProbe:\n",
    "    \"\"\"\n",
    "    Hooks into the model to capture router internals without \n",
    "    changing the model architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, top_k=2):\n",
    "        self.top_k = top_k\n",
    "        self.logs = [] # Stores per-step data\n",
    "        self.layer_names = {}\n",
    "    \n",
    "    def clear(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def register(self, model):\n",
    "        \"\"\"Finds all Gate/Router layers and attaches the hook.\"\"\"\n",
    "        print(f\"Scanning model for routers...\")\n",
    "        count = 0\n",
    "        for name, module in model.named_modules():\n",
    "            # In Qwen1.5-MoE, the router is usually a Linear layer named 'gate'\n",
    "            # inside the MoE block.\n",
    "            if name.endswith(\".gate\"): \n",
    "                self.layer_names[module] = name\n",
    "                module.register_forward_hook(self.hook_fn)\n",
    "                count += 1\n",
    "        print(f\"Attached probes to {count} router layers.\")\n",
    "\n",
    "    def hook_fn(self, module, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Captured during Forward Pass.\n",
    "        Input: Hidden states entering the router\n",
    "        Output: Logits (Raw scores for experts)\n",
    "        \"\"\"\n",
    "        # outputs are the raw logits [batch, seq_len, num_experts]\n",
    "        router_logits = outputs\n",
    "        \n",
    "        # Calculate probabilities\n",
    "        probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Metric: Router Entropy (Uncertainty)\n",
    "        # High entropy = Router is unsure (or load balancing is forcing uniformity)\n",
    "        # Low entropy = Strong specialization\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1).mean()\n",
    "        \n",
    "        # Metric: Expert Activation (Load)\n",
    "        # Manually recalculate Top-K to see which experts won\n",
    "        topk_weights, topk_indices = torch.topk(probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Store lightweight statistics (move to CPU to save VRAM)\n",
    "        step_data = {\n",
    "            \"layer\": self.layer_names[module],\n",
    "            \"entropy\": entropy.item(),\n",
    "            \"active_experts\": topk_indices.flatten().cpu().numpy().tolist()\n",
    "        }\n",
    "        self.logs.append(step_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef6e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental definition\n",
    "\n",
    "@timing\n",
    "def run_experiment(model, tokenizer):\n",
    "    \n",
    "    # Add profiling probe\n",
    "    probe = MoEProbe(top_k=4) # Qwen A2.7B uses Top-4 routing usually (check config)\n",
    "    # Note: Qwen1.5-MoE-A2.7B config: num_experts=60, num_experts_per_tok=4\n",
    "    probe.register(model)\n",
    "    \n",
    "    prompts = {\n",
    "        \"Python Code\": \"def fibonacci(n):\",\n",
    "        \"Creative\": \"The fog rolled into the ancient harbor, smelling of salt and decay.\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for domain, text in prompts.items():\n",
    "        print(f\"\\nTesting Domain: {domain}\")\n",
    "        probe.clear() # Reset logs\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate tokens\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=20)\n",
    "            \n",
    "        # Analyze captured data\n",
    "        print(f\"   captured {len(probe.logs)} routing events.\")\n",
    "        \n",
    "        # Aggregate expert usage for this domain\n",
    "        all_indices = []\n",
    "        avg_entropy = []\n",
    "        for log in probe.logs:\n",
    "            all_indices.extend(log['active_experts'])\n",
    "            avg_entropy.append(log['entropy'])\n",
    "            \n",
    "        results[domain] = {\n",
    "            \"counts\": Counter(all_indices),\n",
    "            \"entropy\": np.mean(avg_entropy)\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a46b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "def plot_results(results):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    \n",
    "    # Expert usage histogram\n",
    "    domain_idx = 0\n",
    "    for domain, data in results.items():\n",
    "        \n",
    "        expert_ids = np.array(sorted(data['counts'].keys()))\n",
    "        counts = np.array([data['counts'][i] for i in expert_ids])\n",
    "        tokens = data['counts'].total()\n",
    "        freqs = counts / tokens\n",
    "        entropy = data['entropy']\n",
    "        bar_width = 0.25\n",
    "        bar_offset = bar_width * domain_idx\n",
    "        ax.bar(expert_ids + bar_offset, freqs, width=bar_width, \\\n",
    "                label='%s, tokens: %d entropy: %2.4f' % (domain, tokens, entropy), \\\n",
    "                alpha=0.7)\n",
    "        domain_idx += 1\n",
    "    \n",
    "    ax.set_title(\"Expert Activation Frequency (Load)\")\n",
    "    ax.set_xlabel(\"Expert Index (0-59)\")\n",
    "    ax.set_ylabel(\"Activation Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Entropy comparison\n",
    "    #ax = axes[1]\n",
    "    #domains = list(results.keys())\n",
    "    #entropies = [results[d]['entropy'] for d in domains]\n",
    "    #ax.bar(domains, entropies, color=['blue', 'orange'])\n",
    "    #ax.set_title(\"Router Entropy (Uncertainty)\")\n",
    "    #ax.set_ylabel(\"Mean Entropy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8bbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def load_model_qwen():\n",
    "    \n",
    "    model_id = \"Qwen/Qwen1.5-MoE-A2.7B-Chat\"\n",
    "    print(f\"Loading {model_id}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        #device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        #torch_dtype=torch.float16 \n",
    "    )\n",
    "    print(\"Model loaded.\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baa913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen1.5-MoE-A2.7B-Chat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591e268b44424fc0b836906887ace12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "func:'load_model_qwen' args:[(), {}] took: 8.6077 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 11.8653 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 11.9456 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.0682 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.1938 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.3022 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.4234 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.5302 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.6348 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.7317 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 12.8407 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.1672 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.0691 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.1702 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.2719 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.3816 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.5098 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.8783 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.7149 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n",
      "   captured 480 routing events.\n",
      "func:'run_experiment' args:[(Qwen2MoeForCausalLM(\n",
      "  (model): Qwen2MoeModel(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2MoeDecoderLayer(\n",
      "        (self_attn): Qwen2MoeSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MoeSparseMoeBlock(\n",
      "          (gate): Linear(in_features=2048, out_features=60, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-59): 60 x Qwen2MoeMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "          )\n",
      "          (shared_expert): Qwen2MoeMLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (act_fn): SiLUActivation()\n",
      "          )\n",
      "          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n",
      "        )\n",
      "        (input_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2MoeRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2MoeRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "), Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-MoE-A2.7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")), {}] took: 13.8260 sec\n",
      "Scanning model for routers...\n",
      "Attached probes to 24 router layers.\n",
      "\n",
      "Testing Domain: Python Code\n",
      "   captured 480 routing events.\n",
      "\n",
      "Testing Domain: Creative\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Main\n",
    "#########\n",
    "\n",
    "# Load model\n",
    "# We select Qwen1.5-MoE-A2.7B-Chat\n",
    "model, tokenizer = load_model_qwen()\n",
    "\n",
    "# Run experiment, repeating n times\n",
    "n = 100\n",
    "results = []\n",
    "for i in range(n):\n",
    "    stats = run_experiment(model, tokenizer)\n",
    "    results.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddead193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack results\n",
    "results_aggregate = {}\n",
    "for domain in results[0]:\n",
    "    results_aggregate[domain] = {\n",
    "        \"counts\": np.sum([results[i][domain][\"counts\"] for i in range(n)]),\n",
    "        \"entropy\": np.mean([results[i][domain][\"entropy\"] for i in range(n)])\n",
    "    }\n",
    "\n",
    "# Plot\n",
    "plot_results(results_aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498fda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
